{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### NOTE: \n",
    "### Before running this: \n",
    "### 1) run git clone https://github.com/jannisborn/covid19_pocus_ultrasound.git in main folder\n",
    "### 2) run python /scripts python3 scripts/cross_val_splitter.py --splits 5 to get training/validation split. \n",
    "\n",
    "### Because of the relatively small size of the dataset, the authors recommend cross-validation. We can either\n",
    "### go with this or set up our own train/val split. Right now I just use the first fold and do 80/20 train/val\n",
    "### for simplicity.\n",
    "\n",
    "\n",
    "### This notebook inputs data, sets up training process, and uses a simple 2-layer CNN as a jank baseline\n",
    "### I currently just do 3-class prediction because it turns out bacterial pheumonia is like\n",
    "### surprisingly different from viral pheumonia \n",
    "### but changing baseline to 2-class and retraining is super easy \n",
    "\n",
    "### Notes about the project:\n",
    "\n",
    "### https://docs.google.com/spreadsheets/d/1t-tLMjMod6W-nAjkuxmO0CLsiyalFIOp92k_XD_yeo8/edit#gid=1181682638\n",
    "### ^^^ all the information about where the ultrasound videos came from, alongside notes taken by \n",
    "### MDs about each dataset. Most of the data is available here and will be used in our project ^^\n",
    "\n",
    "### All images were taken using convex ultrasound probes -- which is great since they offer the best quality\n",
    "### for lung ultrasound (LUS) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4098927/. \n",
    "### Generally more affected by lack of specificity than lack of sensitivity. Raw images all in \n",
    "### data/pocus_images/Convex. It seems like they take 5-6 images from each video -- and\n",
    "### tbh images don't change that much during ultrasound so effectively\n",
    "### a lot of the images are duplicates, which I think should be fine \n",
    "\n",
    "### Other possible sources of images: 1) COVID-19 ultrasound_gallery from Butterfly\n",
    "### which we can download here: https://butterflynetwork.com/covid19/covid-19-ultrasound-gallery.\n",
    "### original repo used this one\n",
    "\n",
    "### 2) Mew ultrasound data (like, 5/16 new) available from The ICLUS project here https://www.disi.unitn.it/iclus\n",
    "### which has 60 more patients' of data, if we want to be reaaaally state of the art. \n",
    "### I requested an account for access in case we need more novelty lol. Surprisingly they\n",
    "### approved it within like 3 hours, no questions asked -- go italy\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "DATA_DIR = 'covid19_pocus_ultrasound/data/cross_validation'\n",
    "MODEL_DIR = 'covid19_pocus_ultrasound/models'\n",
    "\n",
    "### Note: This is what the authors resized images to, we can play with this (or change it)\n",
    "### all depends on what resolution our pretrained model uses \n",
    "\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "\n",
    "\n",
    "### This is which \"fold\" of the cross validation. Rn I just set this as the first one so we do normal\n",
    "### 80/20 train/val split, but we can iterate over this for robust n-fold validation\n",
    "FOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data for getting images from https://github.com/jrosebr1/imutils/blob/master/imutils/paths.py\n",
    "image_types = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "\n",
    "def list_images(basePath, contains=None):\n",
    "    # return the set of files that are valid\n",
    "    return list_files(basePath, validExts=image_types, contains=contains)\n",
    "\n",
    "\n",
    "def list_files(basePath, validExts=None, contains=None):\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if validExts is None or ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename)\n",
    "                yield imagePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "['covid19_pocus_ultrasound/data/cross_validation/split0/covid/Cov-Atlas+(44).gif_frame0.jpg', 'covid19_pocus_ultrasound/data/cross_validation/split0/covid/Cov-Atlas+(44).gif_frame12.jpg', 'covid19_pocus_ultrasound/data/cross_validation/split0/covid/Cov-Atlas+(44).gif_frame15.jpg', 'covid19_pocus_ultrasound/data/cross_validation/split0/covid/Cov-Atlas+(44).gif_frame18.jpg', 'covid19_pocus_ultrasound/data/cross_validation/split0/covid/Cov-Atlas+(44).gif_frame21.jpg']\n"
     ]
    }
   ],
   "source": [
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print('Loading images...')\n",
    "imagePaths = list(list_images(DATA_DIR))\n",
    "\n",
    "train_labels, test_labels = [], []\n",
    "train_data, test_data = [], []\n",
    "\n",
    "print(imagePaths[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training samples: 701 \n",
      "Number of validation samples: 210\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop over folds\n",
    "for imagePath in imagePaths:\n",
    "\n",
    "    path_parts = imagePath.split(os.path.sep)\n",
    "    \n",
    "    # extract the split\n",
    "    train_test = path_parts[-3][-1]\n",
    "    \n",
    "    # extract the class label from the filename\n",
    "    label = path_parts[-2]\n",
    "    \n",
    "    # load the image, swap color channels, and resize it to be a fixed\n",
    "    # 224x224 pixels while ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    # update the data and labels lists, respectively\n",
    "    if train_test == str(FOLD):\n",
    "        test_labels.append(label)\n",
    "        test_data.append(image)\n",
    "    else:\n",
    "        train_labels.append(label)\n",
    "        train_data.append(image)\n",
    "\n",
    "# Prepare data for model\n",
    "print(\n",
    "    f'\\nNumber of training samples: {len(train_labels)} \\n'\n",
    "    f'Number of validation samples: {len(test_labels)}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "### Tbh... why are there 3 channels lol isn't ultrasound all gray anyway\n",
    "print(train_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['covid', 'pneumonia', 'regular'])\n",
      "dict_values([287, 219, 195])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "### ok so these are all strings.. great\n",
    "print(Counter(train_labels).keys())\n",
    "print(Counter(train_labels).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We define a mapping of labels from strings to ints\n",
    "\n",
    "from sklearn import preprocessing\n",
    "labEncoder = preprocessing.LabelEncoder()\n",
    "\n",
    "labEncoder.fit(np.unique(train_labels))\n",
    "\n",
    "train_num_labels = labEncoder.transform(train_labels)\n",
    "test_num_labels = labEncoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BASELINE\n",
    "\n",
    "### Simple 2-layer convnet\n",
    "### Modified from: https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, Sigmoid\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "\n",
    "class BaselineConvNet(Module):   \n",
    "    def __init__(self):\n",
    "        super(BaselineConvNet, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(3, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 2, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        ### Linear layers\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(6272, 100),\n",
    "            ReLU(inplace=True),\n",
    "            ### We have 3 classes but authors recommend adding a 4th class for \"unrelated\"\n",
    "            ### where they make sure if you input like a cat the classifier doesn't say \"healthy lung\" or whatever\n",
    "            ### ... maybe something to talk about in future work lol\n",
    "            Linear(100, 3),\n",
    "            ### No need to add sigmoid because pytorch crossentropyloss apparently includes softmax already\n",
    "            #Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineConvNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=6272, out_features=100, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = BaselineConvNet()\n",
    "\n",
    "# defining the optimizer -- I add some random number for regularization because it makes me feel like I paid\n",
    "# attention in 229\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training with early stopping. Stochastic gradient descent with 300 samples each time\n",
    "\n",
    "def train(epoch, patience, last_loss_val):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    \n",
    "    # clearing the gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ### gets 300 images\n",
    "    inds = np.random.choice(x_train.shape[0],300,replace=False)\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train[inds])\n",
    "    output_val = model(x_val)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train[inds])\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "    \n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    \n",
    "    ### we did not improve, mark it\n",
    "    if loss_val.item() > last_loss_val:\n",
    "        patience += 1\n",
    "    else: \n",
    "        patience = 0\n",
    "    \n",
    "    # printing the train/validation loss\n",
    "    print('Epoch : ',epoch+1, '\\t', 'train loss :', loss_train.item(), '\\t', 'val loss :', loss_val.item(), '\\t', 'patience :', patience)\n",
    "    \n",
    "    ### make sure that we are updating our best val loss so far\n",
    "    if loss_val > last_loss_val:\n",
    "        return patience, last_loss_val\n",
    "    else: \n",
    "        return patience, loss_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set into a form that pytorch likes\n",
    "x_train = torch.tensor(train_data)\n",
    "y_train = torch.tensor(train_num_labels)\n",
    "    \n",
    "# getting the validation set into a form that pytorch likes\n",
    "x_val = torch.tensor(test_data)\n",
    "y_val = torch.tensor(test_num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([701, 3, 224, 224])\n",
      "torch.Size([210, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# pytorch demands that color channels go right after batch size\n",
    "x_train = x_train.permute(0,3,1,2)\n",
    "x_val = x_val.permute(0,3,1,2)\n",
    "\n",
    "## should be (n,3,IMG_SIZE,IMG_SIZE)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.float()\n",
    "x_val = x_val.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t train loss : 1.1569968461990356 \t val loss : 1.197439193725586 \t patience : 0\n",
      "Epoch :  2 \t train loss : 1.6399946212768555 \t val loss : 1.5393999814987183 \t patience : 1\n",
      "Epoch :  3 \t train loss : 1.0631277561187744 \t val loss : 1.0242666006088257 \t patience : 0\n",
      "Epoch :  4 \t train loss : 0.6448227167129517 \t val loss : 0.770695149898529 \t patience : 0\n",
      "Epoch :  5 \t train loss : 0.645271897315979 \t val loss : 0.929279088973999 \t patience : 1\n",
      "Epoch :  6 \t train loss : 0.6078309416770935 \t val loss : 0.8254156708717346 \t patience : 2\n",
      "Epoch :  7 \t train loss : 0.5118973851203918 \t val loss : 0.6162871718406677 \t patience : 0\n",
      "Epoch :  8 \t train loss : 0.4033872187137604 \t val loss : 0.5111725330352783 \t patience : 0\n",
      "Epoch :  9 \t train loss : 0.37352797389030457 \t val loss : 0.49578309059143066 \t patience : 0\n",
      "Epoch :  10 \t train loss : 0.32141125202178955 \t val loss : 0.5095778107643127 \t patience : 1\n",
      "Epoch :  11 \t train loss : 0.3085569143295288 \t val loss : 0.5206810832023621 \t patience : 2\n",
      "Epoch :  12 \t train loss : 0.21723319590091705 \t val loss : 0.5093690156936646 \t patience : 3\n",
      "Epoch :  13 \t train loss : 0.20244412124156952 \t val loss : 0.4802759885787964 \t patience : 0\n",
      "Epoch :  14 \t train loss : 0.20393699407577515 \t val loss : 0.45137259364128113 \t patience : 0\n",
      "Epoch :  15 \t train loss : 0.17450010776519775 \t val loss : 0.437828928232193 \t patience : 0\n",
      "Epoch :  16 \t train loss : 0.1384800523519516 \t val loss : 0.4388691782951355 \t patience : 1\n",
      "Epoch :  17 \t train loss : 0.13455744087696075 \t val loss : 0.44952255487442017 \t patience : 2\n",
      "Epoch :  18 \t train loss : 0.11509975790977478 \t val loss : 0.47276854515075684 \t patience : 3\n",
      "Epoch :  19 \t train loss : 0.11522036045789719 \t val loss : 0.5112749934196472 \t patience : 4\n",
      "Epoch :  20 \t train loss : 0.0839514210820198 \t val loss : 0.5638723969459534 \t patience : 5\n"
     ]
    }
   ],
   "source": [
    "# defining the number of epochs (say, 100)\n",
    "# takes like 15 mins to train lol\n",
    "\n",
    "n_epochs = 100\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "\n",
    "last_loss_val = 1e12\n",
    "patience = 0\n",
    "\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    patience, last_loss_val = train(epoch, patience, last_loss_val)\n",
    "    # if validation doesn't improve within 5 episodes, we quit\n",
    "    if (patience == 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8095238095238095"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### no grad because we're just predicting\n",
    "with torch.no_grad():\n",
    "    output = model(x_val)\n",
    "    \n",
    "### gets the highest predicted class value (out of 2) for each image\n",
    "softmax = torch.exp(output)\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "### accuracy on validation set (62%, 3 classes -- not horrible lol)\n",
    "accuracy_score(y_val, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
